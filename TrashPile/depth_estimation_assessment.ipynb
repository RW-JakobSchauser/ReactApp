{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DPTForDepthEstimation were not initialized from the model checkpoint at Intel/dpt-large and are newly initialized: ['neck.fusion_stage.layers.0.residual_layer1.convolution1.weight', 'neck.fusion_stage.layers.0.residual_layer1.convolution1.bias', 'neck.fusion_stage.layers.0.residual_layer1.convolution2.bias', 'neck.fusion_stage.layers.0.residual_layer1.convolution2.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DPTImageProcessor, DPTForDepthEstimation, GLPNFeatureExtractor, GLPNForDepthEstimation\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "processor = DPTImageProcessor.from_pretrained(\"Intel/dpt-large\")\n",
    "model = DPTForDepthEstimation.from_pretrained(\"Intel/dpt-large\")\n",
    "\n",
    "# feature_extractor = GLPNFeatureExtractor.from_pretrained(\"vinvino02/glpn-kitti\")\n",
    "# model = GLPNForDepthEstimation.from_pretrained(\"vinvino02/glpn-kitti\")\n",
    "# feature_extractor = GLPNFeatureExtractor.from_pretrained(\"vinvino02/glpn-nyu\")\n",
    "# model = GLPNForDepthEstimation.from_pretrained(\"vinvino02/glpn-nyu\")\n",
    "\n",
    "def predict(image):\n",
    "    # prepare image for the model\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    # inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "    # get prediction\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predicted_depth = outputs.predicted_depth\n",
    "\n",
    "    # interpolate to original size\n",
    "    prediction = torch.nn.functional.interpolate(\n",
    "        predicted_depth.unsqueeze(1),\n",
    "        size=image.size[::-1],\n",
    "        mode=\"bicubic\",\n",
    "        align_corners=False,\n",
    "    )\n",
    "\n",
    "    # visualize the prediction\n",
    "    output = prediction.squeeze().cpu().numpy()\n",
    "    formatted = (output * 255 / np.max(output)).astype(\"uint8\")\n",
    "    return formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigpile.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jakhs\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:3631: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "def cutout(image):\n",
    "    image = np.array(image)\n",
    "    cropped = image[0:1000, 420:1000]\n",
    "    return cropped\n",
    "\n",
    "predictions = []\n",
    "for img in os.listdir(\"test_imgs/\"):\n",
    "    print(img)\n",
    "    image = Image.open(\"test_imgs/\" + img)\n",
    "    image = cutout(image)\n",
    "    depth = predict(Image.fromarray(image))\n",
    "    predictions.append(depth)\n",
    "    # save image\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_center_height(contour):\n",
    "    M = cv2.moments(contour)\n",
    "    cx = int(M['m10']/M['m00'])\n",
    "    cy = int(M['m01']/M['m00'])\n",
    "    return cy\n",
    "\n",
    "def find_bounding_rect_area(contour):\n",
    "    x,y,w,h = cv2.boundingRect(contour)\n",
    "    return w*h\n",
    "\n",
    "def find_highest_point(contour):\n",
    "    x,y,w,h = cv2.boundingRect(contour)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contours found\n",
      "(791, 1160, 3)\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os \n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def analyse_frame(image):\n",
    "    depth = predict(Image.fromarray(image))\n",
    "\n",
    "    # depth[depth < 100] = 0\n",
    "\n",
    "\n",
    "    # copy the depth image as we will need it later\n",
    "    depth_copy = depth.copy()\n",
    "    depth_copy = cv2.cvtColor(depth_copy, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "    depth = cv2.cvtColor(depth, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "    sobel_X = cv2.Sobel(depth, cv2.CV_64F, 1, 0, ksize=3) \n",
    "\n",
    "    sobel_X_abs = np.uint8(np.absolute(sobel_X)) \n",
    "\n",
    "\n",
    "    sobel_Y = cv2.Sobel(depth, cv2.CV_64F,0, 1, ksize=3) \n",
    "\n",
    "    sobel_Y_abs = np.uint8(np.absolute(sobel_Y)) \n",
    "\n",
    "\n",
    "    depth = cv2.bitwise_or(sobel_Y_abs,sobel_X_abs) \n",
    "    # depth = sobel_Y_abs\n",
    "    # depth = cv2.line(depth, (200, i), (400, i), (0, 0, 255), 2)\n",
    "\n",
    " \n",
    "    # find the main edge of the mountain using canny edge detection\n",
    "\n",
    "\n",
    "    depth = cv2.Canny(depth, 100, 200)\n",
    "\n",
    "    blurred = cv2.GaussianBlur(depth, (3, 3), 0)\n",
    "\n",
    "\n",
    "    # # # Find contours in the depth \n",
    "    contours, hierarchy = cv2.findContours(blurred, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE) \n",
    "    # # # Select the contour you want to extract \n",
    "    # # For example, you can select the largest contour \n",
    "\n",
    "    # # find the centroids of the contours\n",
    "    contours = list(filter(lambda cnt: find_center_height(cnt) > 0 and find_center_height(cnt) < 600, contours))\n",
    "\n",
    "    if len (contours) != 0:\n",
    "        print(\"contours found\")\n",
    "        largest_contour = max(contours, key=find_bounding_rect_area)\n",
    "\n",
    "\n",
    "    # depth = cv2.cvtColor(depth, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "    #  # Draw the selected contour on a blank depth \n",
    "\n",
    "    # find the highest point of the contour\n",
    "\n",
    "        i = find_highest_point(largest_contour)\n",
    "        \n",
    "        image = cv2.line(image, (200, i), (400, i), (0, 0, 255), 2)\n",
    "\n",
    "        top, bottom = depth_copy.shape[0], 400\n",
    "\n",
    "        height_percentage = (1 - (i + 0.1)/(top - bottom ))\n",
    "        \n",
    "        c = (255, 0, 0)\n",
    "        note = \"\"\n",
    "        # draw text on the image\n",
    "        if height_percentage > 0.90:\n",
    "            c = (0, 0, 255) \n",
    "            note = \"Critical level!\"\n",
    "        elif height_percentage > 0.75:\n",
    "            # yellow\n",
    "            c = (0, 255, 255)\n",
    "            note = \"Soon to be full\"\n",
    "        elif height_percentage > 0.4:\n",
    "            note = \"Slowly filling up\"\n",
    "        \n",
    "\n",
    "        textx = 200\n",
    "        texty = 500\n",
    "        cv2.putText(depth_copy, f\"Capacity: {((height_percentage)):.0%}\", (textx, texty), cv2.FONT_HERSHEY_SIMPLEX, 1, c, 2, cv2.LINE_AA)\n",
    "        cv2.putText(depth_copy, f\"{note}\", (textx, texty+25), cv2.FONT_HERSHEY_SIMPLEX, 1, c, 2, cv2.LINE_AA)\n",
    "\n",
    "\n",
    "            # draw lines at 200 and 600\n",
    "        # depth_copy = cv2.line(depth_copy, (0, 200), (1000, 200), (255, 255, 255), 2)\n",
    "        # depth_copy = cv2.line(depth_copy, (0, 600), (1000, 600), (255, 255, 255), 2)\n",
    "        \n",
    "        cv2.drawContours(depth_copy, [largest_contour], -1, (200, 255, 200), 2)\n",
    "\n",
    "    toshow = np.concatenate((image, depth_copy), axis=1)\n",
    "\n",
    "    print(toshow.shape)\n",
    "    return toshow\n",
    "    cv2.imshow('frame', toshow)\n",
    "\n",
    "for i, img in enumerate(os.listdir(\"test_imgs/\")):\n",
    "    image = cv2.imread(\"test_imgs/\" + img)\n",
    "    image = cutout(image)\n",
    "    frame = analyse_frame(image)\n",
    "    # open the image with cv2\n",
    "    cv2.imshow('frame', frame)\n",
    "    cv2.waitKey(0)\n",
    "    break\n",
    "  \n",
    "cv2.destroyAllWindows() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contours found\n",
      "(1000, 1240, 3)\n",
      "actual (1000, 1240, 3)\n",
      "contours found\n",
      "(1000, 1240, 3)\n",
      "actual (1000, 1240, 3)\n",
      "contours found\n",
      "(1000, 1240, 3)\n",
      "actual (1000, 1240, 3)\n",
      "contours found\n",
      "(1000, 1240, 3)\n",
      "actual (1000, 1240, 3)\n",
      "broke\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# open the video file\n",
    "import cv2\n",
    "# import threading\n",
    "import time\n",
    "# loop through the video\n",
    "\n",
    "\n",
    "\n",
    "def show():\n",
    "    cap = cv2.VideoCapture('videos/trashpile_combined.mp4')\n",
    "\n",
    "    # get the frame width and height\n",
    "    width = int(1240)\n",
    "    height = int(1000)\n",
    "\n",
    "    # create VideoWriter object\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    out = cv2.VideoWriter('output_combined.avi', fourcc, 10, (width, height))\n",
    "\n",
    "    # get the frames per second\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    # get the frame width and height\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    predictions = []\n",
    "    i = 0\n",
    "    total_i = 0\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            i += 1\n",
    "            if i % 30 != 0:\n",
    "                continue\n",
    "            # start a thread to analyze the frame\n",
    "            frame = cutout(frame)\n",
    "            frame = analyse_frame(frame)\n",
    "\n",
    "            cv2.imshow('frame', frame)\n",
    "\n",
    "            print(\"actual\", frame.shape)\n",
    "            # write to video\n",
    "            # out.write(frame)\n",
    "            # press q to quit\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                print(\"broke\")\n",
    "                break\n",
    "        else:\n",
    "            print(\"no frame\")\n",
    "            break\n",
    "\n",
    "    # release the VideoCapture object\n",
    "    print(\"done\")\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
