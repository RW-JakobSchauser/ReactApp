{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "import os \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from transformers import OwlViTProcessor, OwlViTForObjectDetection\n",
    "from PIL import ImageDraw\n",
    "from PIL import ImageFont\n",
    "\n",
    "processor = OwlViTProcessor.from_pretrained(\"google/owlvit-base-patch32\")\n",
    "model = OwlViTForObjectDetection.from_pretrained(\"google/owlvit-base-patch32\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stand_texts = [[\"a hill\", \"a mountain\", \"a peak\", \"a pile of gravel\", \"a pyramid\", \"a triangle of dirt\", \"a mound of dirt\", \"a cone\", \"a mound of trash\", \"a pile of rocks\"]]\n",
    "\n",
    "t_small = [ text.split(\" \")[0] + \" small \" + \" \".join(text.split(\" \")) for text in stand_texts[0]]\n",
    "t_large = [ text.split(\" \")[0] + \" large \" + \" \".join(text.split(\" \")) for text in stand_texts[0]]\n",
    "\n",
    "texts = [t_small]#[stand_texts[0] + t_small + t_large]\n",
    "\n",
    "\n",
    "def cutout(image):\n",
    "    image = np.array(image)\n",
    "    cropped = image[0:1000, 600:1400]\n",
    "    return cropped\n",
    "\n",
    "def analyse_frame(frame, last_height = (False, 0)):\n",
    "    global texts\n",
    "    image = Image.fromarray(cutout(frame))\n",
    "    inputs = processor(text=texts, images=image, return_tensors=\"pt\")\n",
    "\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    target_sizes = torch.Tensor([image.size[::-1]])\n",
    "\n",
    "    results = processor.post_process(outputs=outputs, target_sizes=target_sizes)\n",
    "\n",
    "    i = 0  \n",
    "    text = texts[i]\n",
    "    boxes, scores, labels = results[i][\"boxes\"], results[i][\"scores\"], results[i][\"labels\"]\n",
    "\n",
    "    # sort the boxes and associated scores in descending order\n",
    "    scores, idxs = scores.sort(descending=True)\n",
    "    scores  = scores.detach().numpy()\n",
    "    boxes   = boxes[idxs].detach().numpy()\n",
    "    labels  = labels[idxs].detach().numpy()\n",
    "    draw    = ImageDraw.Draw(image)\n",
    "\n",
    "    # scores  = scores[:5]\n",
    "    # boxes   = boxes[:5]\n",
    "\n",
    "    # # average over all boxes\n",
    "    # print(scores)\n",
    "    # box = np.average(boxes, axis=0)#, weights = scores)\n",
    "\n",
    "\n",
    "\n",
    "    box = boxes[0]\n",
    "\n",
    "    height = box[1]\n",
    "    if last_height[0]:\n",
    "        height = (height + last_height[1]) / 2\n",
    "        box[1] = height\n",
    "    # height = box[1]\n",
    "    # for box, score, label in zip(boxes, scores, labels):\n",
    "    if height < 50:\n",
    "        txt = \"Overfull!!\"\n",
    "        color = \"red\"\n",
    "    elif height < 200:\n",
    "        txt = \"Filling up!\"\n",
    "        color = \"orange\"\n",
    "    else:\n",
    "        txt = \"All good :)\"\n",
    "        color = \"green\"\n",
    "\n",
    "    #     print(texts[i][label.numpy()])\n",
    "    draw.rectangle(box, outline= color, width=10)\n",
    "\n",
    "    # draw text on the image\n",
    "    draw.text((300, 700), text= txt , fill=color, font=ImageFont.truetype(\"arial\", 75))\n",
    "\n",
    "    return image, height\n",
    "\n",
    "# for img in os.listdir(\"test_imgs\"):\n",
    "\n",
    "#     image = Image.open(\"test_imgs/\" + img)\n",
    "#     print(image.size)\n",
    "#     plt.imshow(image)\n",
    "#     # break\n",
    "\n",
    "#     # texts = [[\"a pile\", \"pile\"]]\n",
    "#     image = analyse_frame(image)\n",
    "    \n",
    "    \n",
    "#     # draw bounding box on image\n",
    "\n",
    "#     plt.imshow(image)\n",
    "#     # break\n",
    "#     plt.show()\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jakhs\\Anaconda3\\lib\\site-packages\\transformers\\models\\owlvit\\image_processing_owlvit.py:355: FutureWarning: `post_process` is deprecated and will be removed in v5 of Transformers, please use `post_process_object_detection`\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual (1000, 800, 3)\n",
      "actual (1000, 800, 3)\n",
      "actual (1000, 800, 3)\n",
      "actual (1000, 800, 3)\n",
      "actual (1000, 800, 3)\n",
      "actual (1000, 800, 3)\n",
      "actual (1000, 800, 3)\n",
      "actual (1000, 800, 3)\n",
      "actual (1000, 800, 3)\n",
      "actual (1000, 800, 3)\n",
      "actual (1000, 800, 3)\n",
      "actual (1000, 800, 3)\n",
      "actual (1000, 800, 3)\n",
      "actual (1000, 800, 3)\n",
      "actual (1000, 800, 3)\n",
      "actual (1000, 800, 3)\n",
      "actual (1000, 800, 3)\n",
      "actual (1000, 800, 3)\n",
      "actual (1000, 800, 3)\n",
      "actual (1000, 800, 3)\n",
      "actual (1000, 800, 3)\n",
      "actual (1000, 800, 3)\n",
      "no frame\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "def show():\n",
    "    cap = cv2.VideoCapture('videos/trashpile_combined.mp4')\n",
    "\n",
    "    # get the frame width and height\n",
    "    width = int(800)\n",
    "    height = int(1000)\n",
    "\n",
    "    # create VideoWriter object\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    out = cv2.VideoWriter('output_oneshot.avi', fourcc, 3, (width, height))\n",
    "\n",
    "    # get the frames per second\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    # get the frame width and height\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    predictions = []\n",
    "    i = 0\n",
    "    total_i = 0\n",
    "    last_height = (False, 0)\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            i += 1\n",
    "            if i % 20 != 0:\n",
    "                continue\n",
    "            # start a thread to analyze the frame\n",
    "            # frame = cutout(frame)\n",
    "            frame, height = analyse_frame(frame, last_height)\n",
    "            last_height = (True, height)\n",
    "            # convert from PIL image to OpenCV image\n",
    "            frame = np.array(frame)\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "            # show the frame\n",
    "\n",
    "\n",
    "            # cv2.imshow('frame', frame)\n",
    "\n",
    "            print(\"actual\", frame.shape)\n",
    "            # write to video\n",
    "            out.write(frame)\n",
    "            # press q to quit\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                print(\"broke\")\n",
    "                break\n",
    "        else:\n",
    "            print(\"no frame\")\n",
    "            break\n",
    "\n",
    "    # release the VideoCapture object\n",
    "    print(\"done\")\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
